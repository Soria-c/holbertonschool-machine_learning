


![Project badge](/assets/pathway/004_color-d2fbcfb42ba7481834896ecc89a6f0ae65762b3c1096691dd0f820f7f29e3389.png)83\.83%# Hidden Markov Models

* Master
* By: Alexa Orrico, Software Engineer at Holberton School
* Weight: 5
* Your score will be updated as you progress.




* [Description](#description)





[Go to tasks](#)

![](https://s3.eu-west-3.amazonaws.com/hbtn.intranet/uploads/medias/2020/1/027d4a67aea17e6fa181.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA4MYA5JM5DUTZGMZG%2F20241026%2Feu-west-3%2Fs3%2Faws4_request&X-Amz-Date=20241026T024630Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e191aebc2ba6506fbdd3509f2c6fc91fe0eee4ef6165c552914b062301eb624b)


## Resources


**Read or watch**:


* [Markov property](/rltoken/TAtRYhs7AO2TJyj39wuvaA "Markov property")
* [Markov Chain](/rltoken/Yy4IKZ9g_Nem66zllHqsLA "Markov Chain")
* [Properties of Markov Chains](/rltoken/Evj7IhQr3Iyv5g7vhr3Qcw "Properties of Markov Chains")
* [Markov Chains](/rltoken/uQ0lzukhz5lcBaGfLrnxww "Markov Chains")
* [Markov Matrices](/rltoken/aGA5WovI84GMN2w7uZ3VwQ "Markov Matrices")
* [1\.3 Convergence of Regular Markov Chains](/rltoken/6HhIKgvcdFCXvevpe3q34Q "1.3 Convergence of Regular Markov Chains")
* [Markov Chains, Part 1](/rltoken/c-NtiXn-SgRZr8UxGrKhqg "Markov Chains, Part 1")
* [Markov Chains, Part 2](/rltoken/smxrL-0ZFW8St9HR94TunQ "Markov Chains, Part 2")
* [Markov Chains, Part 3](/rltoken/ryJzuh3TnlFRTYnKu6RQqw "Markov Chains, Part 3")
* [Markov Chains, Part 4](/rltoken/Zv3uhcjw9ySzjpors5uVxg "Markov Chains, Part 4")
* [Markov Chains, Part 5](/rltoken/VFq1SzanqeQ1SEU0Pd3Akw "Markov Chains, Part 5")
* [Markov Chains, Part 7](/rltoken/ZvikOu-Mj3VF5gpUdy0trQ "Markov Chains, Part 7")
* [Markov Chains, Part 8](/rltoken/DVkHyO0_1eAtXh8_WqEqUw "Markov Chains, Part 8")
* [Markov Chains, Part 9](/rltoken/rJWeQ2wYBxaM5bNI65VkHw "Markov Chains, Part 9")
* [Hidden Markov model](/rltoken/BEwIeSOFelODdr52QP1CxQ "Hidden Markov model")
* [Hidden Markov Models](/rltoken/5BrZuRac3-uMZ42j_2HZnA "Hidden Markov Models")
* [(ML 14\.1\) Markov models \- motivating examples](/rltoken/oSoWpD1mNsaF7hN_wb7tgw "(ML 14.1) Markov models - motivating examples")
* [(ML 14\.2\) Markov chains (discrete\-time) (part 1\)](/rltoken/kkugajXgj0rvLM_1Ql8zEA "(ML 14.2) Markov chains (discrete-time) (part 1)")
* [(ML 14\.3\) Markov chains (discrete\-time) (part 2\)](/rltoken/TGUMOg2fYzgG6lel4QZl8w "(ML 14.3) Markov chains (discrete-time) (part 2)")
* [(ML 14\.4\) Hidden Markov models (HMMs) (part 1\)](/rltoken/zeMDX6YbcGH9TENYwJ36TQ "(ML 14.4) Hidden Markov models (HMMs) (part 1)")
* [(ML 14\.5\) Hidden Markov models (HMMs) (part 2\)](/rltoken/xDnmwfjLdYNgeR1sbVIONg "(ML 14.5) Hidden Markov models (HMMs) (part 2)")
* [(ML 14\.6\) Forward\-Backward algorithm for HMMs](/rltoken/zqQR_LloNDMO-G-mFl01gg "(ML 14.6) Forward-Backward algorithm for HMMs")
* [(ML 14\.7\) Forward algorithm (part 1\)](/rltoken/s5wqei6O3POe6_kvpQVxzg "(ML 14.7) Forward algorithm (part 1)")
* [(ML 14\.8\) Forward algorithm (part 2\)](/rltoken/mBgq0a_WOjj7gw8ahp4vBQ "(ML 14.8) Forward algorithm (part 2)")
* [(ML 14\.9\) Backward algorithm](/rltoken/RspxdIyL8MLDX98bYoIk0A "(ML 14.9) Backward algorithm")
* [(ML 14\.10\) Underflow and the log\-sum\-exp trick](/rltoken/PhmpVDURQBesqPDcImTt9Q "(ML 14.10) Underflow and the log-sum-exp trick")
* [(ML 14\.11\) Viterbi algorithm (part 1\)](/rltoken/E4SanY4DdI7HBbwnE4IKNg "(ML 14.11) Viterbi algorithm (part 1)")
* [(ML 14\.12\) Viterbi algorithm (part 2\)](/rltoken/M5giuaddCz96Tc8zrzcBzg "(ML 14.12) Viterbi algorithm (part 2)")


## Learning Objectives


* What is the Markov property?
* What is a Markov chain?
* What is a state?
* What is a transition probability/matrix?
* What is a stationary state?
* What is a regular Markov chain?
* How to determine if a transition matrix is regular
* What is an absorbing state?
* What is a transient state?
* What is a recurrent state?
* What is an absorbing Markov chain?
* What is a Hidden Markov Model?
* What is a hidden state?
* What is an observation?
* What is an emission probability/matrix?
* What is a Trellis diagram?
* What is the Forward algorithm and how do you implement it?
* What is decoding?
* What is the Viterbi algorithm and how do you implement it?
* What is the Forward\-Backward algorithm and how do you implement it?
* What is the Baum\-Welch algorithm and how do you implement it?


## Requirements


### General


* Allowed editors: `vi`, `vim`, `emacs`
* All your files will be interpreted/compiled on Ubuntu 20\.04 LTS using `python3` (version 3\.9\)
* Your files will be executed with `numpy` (version 1\.25\.2\)
* All your files should end with a new line
* The first line of all your files should be exactly `#!/usr/bin/env python3`
* A `README.md` file, at the root of the folder of the project, is mandatory
* Your code should use the `pycodestyle` style (version 2\.11\.1\)
* All your modules should have documentation (`python3 -c 'print(__import__("my_module").__doc__)'`)
* All your classes should have documentation (`python3 -c 'print(__import__("my_module").MyClass.__doc__)'`)
* All your functions (inside and outside a class) should have documentation (`python3 -c 'print(__import__("my_module").my_function.__doc__)'` and `python3 -c 'print(__import__("my_module").MyClass.my_function.__doc__)'`)
* Unless otherwise noted, you are not allowed to import any module except `import numpy as np`
* All your files must be executable







## Tasks







### 0\. Markov Chain




 mandatory
 











 Score: 100\.00% (Checks completed: 100\.00%)
 


Write the function `def markov_chain(P, s, t=1):` that determines the probability of a markov chain being in a particular state after a specified number of iterations:


* `P` is a square 2D `numpy.ndarray` of shape `(n, n)` representing the transition matrix
	+ `P[i, j]` is the probability of transitioning from state `i` to state `j`
	+ `n` is the number of states in the markov chain
* `s` is a `numpy.ndarray` of shape `(1, n)` representing the probability of starting in each state
* `t` is the number of iterations that the markov chain has been through
* Returns: a `numpy.ndarray` of shape `(1, n)` representing the probability of being in a specific state after `t` iterations, or `None` on failure



```
alexa@ubuntu-xenial:0x02-hmm$ cat 0-main.py
	#!/usr/bin/env python3
	
	import numpy as np
	markov_chain = __import__('0-markov_chain').markov_chain
	
	if __name__ == "__main__":
		P = np.array([[0.25, 0.2, 0.25, 0.3], [0.2, 0.3, 0.2, 0.3], [0.25, 0.25, 0.4, 0.1], [0.3, 0.3, 0.1, 0.3]])
		s = np.array([[1, 0, 0, 0]])
		print(markov_chain(P, s, 300))
	alexa@ubuntu-xenial:0x02-hmm$ ./0-main.py
	[[0.2494929  0.26335362 0.23394185 0.25321163]]
	alexa@ubuntu-xenial:0x02-hmm$
	
```






**Repo:**


* GitHub repository: `holbertonschool-machine_learning`
* Directory: `unsupervised_learning/hmm`
* File: `0-markov_chain.py`










 Help
 




×
#### Students who are done with "0\. Markov Chain"

















 Review your work
 




×
#### Correction of "0\. Markov Chain"







Start a new test
Close


















 Requirement success
 


 Requirement fail
 




 Code success
 


 Code fail
 




 Efficiency success
 


 Efficiency fail
 




 Text answer success
 


 Text answer fail
 




 Skipped \- Previous check failed
 








 QA Review
 




×
#### 0\. Markov Chain












##### Commit used:


* **User:**  \-\-\-
* **URL:** Click here
* **ID:** `---`
* **Author:** \-\-\-
* **Subject:** *\-\-\-*
* **Date:** \-\-\-














**3/3** 
pts









### 1\. Regular Chains




 mandatory
 











 Score: 100\.00% (Checks completed: 100\.00%)
 


Write the function `def regular(P):` that determines the steady state probabilities of a regular markov chain:


* `P` is a is a square 2D `numpy.ndarray` of shape `(n, n)` representing the transition matrix
	+ `P[i, j]` is the probability of transitioning from state `i` to state `j`
	+ `n` is the number of states in the markov chain
* Returns: a `numpy.ndarray` of shape `(1, n)` containing the steady state probabilities, or `None` on failure



```
alexa@ubuntu-xenial:0x02-hmm$ cat 1-main.py
	#!/usr/bin/env python3
	
	import numpy as np
	regular = __import__('1-regular').regular
	
	if __name__ == '__main__':
		a = np.eye(2)
		b = np.array([[0.6, 0.4],
					  [0.3, 0.7]])
		c = np.array([[0.25, 0.2, 0.25, 0.3],
					  [0.2, 0.3, 0.2, 0.3],
					  [0.25, 0.25, 0.4, 0.1],
					  [0.3, 0.3, 0.1, 0.3]])
		d = np.array([[0.8, 0.2, 0, 0, 0],
					[0.25, 0.75, 0, 0, 0],
					[0, 0, 0.5, 0.2, 0.3],
					[0, 0, 0.3, 0.5, .2],
					[0, 0, 0.2, 0.3, 0.5]])
		e = np.array([[1, 0.25, 0, 0, 0],
					[0.25, 0.75, 0, 0, 0],
					[0, 0.1, 0.5, 0.2, 0.2],
					[0, 0.1, 0.2, 0.5, .2],
					[0, 0.1, 0.2, 0.2, 0.5]])
		print(regular(a))
		print(regular(b))
		print(regular(c))
		print(regular(d))
		print(regular(e))
	alexa@ubuntu-xenial:0x02-hmm$ ./1-main.py
	None
	[[0.42857143 0.57142857]]
	[[0.2494929  0.26335362 0.23394185 0.25321163]]
	None
	None
	alexa@ubuntu-xenial:0x02-hmm$ 
	
```






**Repo:**


* GitHub repository: `holbertonschool-machine_learning`
* Directory: `unsupervised_learning/hmm`
* File: `1-regular.py`










 Help
 




×
#### Students who are done with "1\. Regular Chains"

















 Review your work
 




×
#### Correction of "1\. Regular Chains"







Start a new test
Close


















 Requirement success
 


 Requirement fail
 




 Code success
 


 Code fail
 




 Efficiency success
 


 Efficiency fail
 




 Text answer success
 


 Text answer fail
 




 Skipped \- Previous check failed
 








 QA Review
 




×
#### 1\. Regular Chains












##### Commit used:


* **User:**  \-\-\-
* **URL:** Click here
* **ID:** `---`
* **Author:** \-\-\-
* **Subject:** *\-\-\-*
* **Date:** \-\-\-














**7/7** 
pts









### 2\. Absorbing Chains




 mandatory
 











 Score: 100\.00% (Checks completed: 100\.00%)
 


Write the function `def absorbing(P):` that determines if a markov chain is absorbing:


* P is a is a square 2D `numpy.ndarray` of shape `(n, n)` representing the standard transition matrix
	+ `P[i, j]` is the probability of transitioning from state `i` to state `j`
	+ `n` is the number of states in the markov chain
* Returns: `True` if it is absorbing, or `False` on failure



```
alexa@ubuntu-xenial:0x02-hmm$ cat 2-main.py
	#!/usr/bin/env python3
	
	import numpy as np
	absorbing = __import__('2-absorbing').absorbing
	
	if __name__ == '__main__':
		a = np.eye(2)
		b = np.array([[0.6, 0.4],
					  [0.3, 0.7]])
		c = np.array([[0.25, 0.2, 0.25, 0.3],
					  [0.2, 0.3, 0.2, 0.3],
					  [0.25, 0.25, 0.4, 0.1],
					  [0.3, 0.3, 0.1, 0.3]])
		d = np.array([[1, 0, 0, 0, 0],
					  [0.25, 0.75, 0, 0, 0],
					  [0, 0, 0.5, 0.2, 0.3],
					  [0, 0, 0.3, 0.5, .2],
					  [0, 0, 0.2, 0.3, 0.5]])
		e = np.array([[1, 0, 0, 0, 0],
					  [0.25, 0.75, 0, 0, 0],
					  [0, 0.1, 0.5, 0.2, 0.2],
					  [0, 0.1, 0.2, 0.5, .2],
					  [0, 0.1, 0.2, 0.2, 0.5]])
		f = np.array([[1, 0, 0, 0],
					  [0, 1, 0, 0],
					  [0, 0, 0.5, 0.5],
					  [0, 0.5, 0.5, 0]])
		print(absorbing(a))
		print(absorbing(b))
		print(absorbing(c))
		print(absorbing(d))
		print(absorbing(e))
		print(absorbing(f))
	alexa@ubuntu-xenial:0x02-hmm$ ./2-main.py
	True
	False
	False
	False
	True
	True
	alexa@ubuntu-xenial:0x02-hmm$
	
```






**Repo:**


* GitHub repository: `holbertonschool-machine_learning`
* Directory: `unsupervised_learning/hmm`
* File: `2-absorbing.py`










 Help
 




×
#### Students who are done with "2\. Absorbing Chains"

















 Review your work
 




×
#### Correction of "2\. Absorbing Chains"







Start a new test
Close


















 Requirement success
 


 Requirement fail
 




 Code success
 


 Code fail
 




 Efficiency success
 


 Efficiency fail
 




 Text answer success
 


 Text answer fail
 




 Skipped \- Previous check failed
 








 QA Review
 




×
#### 2\. Absorbing Chains












##### Commit used:


* **User:**  \-\-\-
* **URL:** Click here
* **ID:** `---`
* **Author:** \-\-\-
* **Subject:** *\-\-\-*
* **Date:** \-\-\-














**8/8** 
pts









### 3\. The Forward Algorithm




 mandatory
 











 Score: 65\.00% (Checks completed: 100\.00%)
 


![](https://s3.eu-west-3.amazonaws.com/hbtn.intranet/uploads/medias/2020/1/a4a616525a089952d29f.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA4MYA5JM5DUTZGMZG%2F20241026%2Feu-west-3%2Fs3%2Faws4_request&X-Amz-Date=20241026T024630Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=17ec22a188d044c658bfc70b993af99ffb357e6a96389050e681933a05fc87e1)


![](https://s3.eu-west-3.amazonaws.com/hbtn.intranet/uploads/medias/2020/1/f847db61fbc52eda75d9.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA4MYA5JM5DUTZGMZG%2F20241026%2Feu-west-3%2Fs3%2Faws4_request&X-Amz-Date=20241026T024630Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=b4265a868268c53e9644f80bb9b98df6ed74a2604a81041f71a654c750bfae8a)


Write the function `def forward(Observation, Emission, Transition, Initial):` that performs the forward algorithm for a hidden markov model:


* `Observation` is a `numpy.ndarray` of shape `(T,)` that contains the index of the observation
	+ `T` is the number of observations
* `Emission` is a `numpy.ndarray` of shape `(N, M)` containing the emission probability of a specific observation given a hidden state
	+ `Emission[i, j]` is the probability of observing `j` given the hidden state `i`
	+ `N` is the number of hidden states
	+ `M` is the number of all possible observations
* `Transition` is a 2D `numpy.ndarray` of shape `(N, N)` containing the transition probabilities
	+ `Transition[i, j]` is the probability of transitioning from the hidden state `i` to `j`
* `Initial` a `numpy.ndarray` of shape `(N, 1)` containing the probability of starting in a particular hidden state
* Returns: `P, F`, or `None, None` on failure
	+ `P` is the likelihood of the observations given the model
	+ `F` is a `numpy.ndarray` of shape `(N, T)` containing the forward path probabilities
		- `F[i, j]` is the probability of being in hidden state `i` at time `j` given the previous observations



```
alexa@ubuntu-xenial:0x02-hmm$ cat 3-main.py
	#!/usr/bin/env python3
	
	import numpy as np
	forward = __import__('3-forward').forward
	
	if __name__ == '__main__':
		np.random.seed(1)
		Emission = np.array([[0.90, 0.10, 0.00, 0.00, 0.00, 0.00],
							 [0.40, 0.50, 0.10, 0.00, 0.00, 0.00],
							 [0.00, 0.25, 0.50, 0.25, 0.00, 0.00],
							 [0.00, 0.00, 0.05, 0.70, 0.15, 0.10],
							 [0.00, 0.00, 0.00, 0.20, 0.50, 0.30]])
		Transition = np.array([[0.60, 0.39, 0.01, 0.00, 0.00],
							   [0.20, 0.50, 0.30, 0.00, 0.00],
							   [0.01, 0.24, 0.50, 0.24, 0.01],
							   [0.00, 0.00, 0.15, 0.70, 0.15],
							   [0.00, 0.00, 0.01, 0.39, 0.60]])
		Initial = np.array([0.05, 0.20, 0.50, 0.20, 0.05])
		Hidden = [np.random.choice(5, p=Initial)]
		for _ in range(364):
			Hidden.append(np.random.choice(5, p=Transition[Hidden[-1]]))
		Hidden = np.array(Hidden)
		Observations = []
		for s in Hidden:
			Observations.append(np.random.choice(6, p=Emission[s]))
		Observations = np.array(Observations)
		P, F = forward(Observations, Emission, Transition, Initial.reshape((-1, 1)))
		print(P)
		print(F)
	alexa@ubuntu-xenial:0x02-hmm$ ./3-main.py
	1.7080966131859584e-214
	[[0.00000000e+000 0.00000000e+000 2.98125000e-004 ... 0.00000000e+000
	  0.00000000e+000 0.00000000e+000]
	 [2.00000000e-002 0.00000000e+000 3.18000000e-003 ... 0.00000000e+000
	  0.00000000e+000 0.00000000e+000]
	 [2.50000000e-001 3.31250000e-002 0.00000000e+000 ... 2.13885975e-214
	  1.17844112e-214 0.00000000e+000]
	 [1.00000000e-002 4.69000000e-002 0.00000000e+000 ... 2.41642482e-213
	  1.27375484e-213 9.57568349e-215]
	 [0.00000000e+000 8.00000000e-004 0.00000000e+000 ... 1.96973759e-214
	  9.65573676e-215 7.50528264e-215]]
	alexa@ubuntu-xenial:0x02-hmm$
	
```






**Repo:**


* GitHub repository: `holbertonschool-machine_learning`
* Directory: `unsupervised_learning/hmm`
* File: `3-forward.py`










 Help
 




×
#### Students who are done with "3\. The Forward Algorithm"

















 Review your work
 




×
#### Correction of "3\. The Forward Algorithm"







Start a new test
Close


















 Requirement success
 


 Requirement fail
 




 Code success
 


 Code fail
 




 Efficiency success
 


 Efficiency fail
 




 Text answer success
 


 Text answer fail
 




 Skipped \- Previous check failed
 








 QA Review
 




×
#### 3\. The Forward Algorithm












##### Commit used:


* **User:**  \-\-\-
* **URL:** Click here
* **ID:** `---`
* **Author:** \-\-\-
* **Subject:** *\-\-\-*
* **Date:** \-\-\-














**1\.95/3** 
pts









### 4\. The Viretbi Algorithm




 mandatory
 











 Score: 65\.00% (Checks completed: 100\.00%)
 


Write the function `def viterbi(Observation, Emission, Transition, Initial):` that calculates the most likely sequence of hidden states for a hidden markov model:


* `Observation` is a `numpy.ndarray` of shape `(T,)` that contains the index of the observation
	+ `T` is the number of observations
* `Emission` is a `numpy.ndarray` of shape `(N, M)` containing the emission probability of a specific observation given a hidden state
	+ `Emission[i, j]` is the probability of observing `j` given the hidden state `i`
	+ `N` is the number of hidden states
	+ `M` is the number of all possible observations
* `Transition` is a 2D `numpy.ndarray` of shape `(N, N)` containing the transition probabilities
	+ `Transition[i, j]` is the probability of transitioning from the hidden state `i` to `j`
* `Initial` a `numpy.ndarray` of shape `(N, 1)` containing the probability of starting in a particular hidden state
* Returns: `path, P`, or `None, None` on failure
	+ `path` is the a list of length `T` containing the most likely sequence of hidden states
	+ `P` is the probability of obtaining the `path` sequence



```
alexa@ubuntu-xenial:0x02-hmm$ cat 4-main.py
	#!/usr/bin/env python3
	
	import numpy as np
	viterbi = __import__('4-viterbi').viterbi
	
	if __name__ == '__main__':
		np.random.seed(1)
		Emission = np.array([[0.90, 0.10, 0.00, 0.00, 0.00, 0.00],
							 [0.40, 0.50, 0.10, 0.00, 0.00, 0.00],
							 [0.00, 0.25, 0.50, 0.25, 0.00, 0.00],
							 [0.00, 0.00, 0.05, 0.70, 0.15, 0.10],
							 [0.00, 0.00, 0.00, 0.20, 0.50, 0.30]])
		Transition = np.array([[0.60, 0.39, 0.01, 0.00, 0.00],
							   [0.20, 0.50, 0.30, 0.00, 0.00],
							   [0.01, 0.24, 0.50, 0.24, 0.01],
							   [0.00, 0.00, 0.15, 0.70, 0.15],
							   [0.00, 0.00, 0.01, 0.39, 0.60]])
		Initial = np.array([0.05, 0.20, 0.50, 0.20, 0.05])
		Hidden = [np.random.choice(5, p=Initial)]
		for _ in range(364):
			Hidden.append(np.random.choice(5, p=Transition[Hidden[-1]]))
		Hidden = np.array(Hidden)
		Observations = []
		for s in Hidden:
			Observations.append(np.random.choice(6, p=Emission[s]))
		Observations = np.array(Observations)
		path, P = viterbi(Observations, Emission, Transition, Initial.reshape((-1, 1)))
		print(P)
		print(path)
	alexa@ubuntu-xenial:0x02-hmm$ ./4-main.py
	4.701733355108224e-252
	[2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 2, 2, 2, 3, 3, 3, 2, 1, 2, 1, 1, 2, 2, 2, 3, 3, 2, 2, 3, 4, 4, 3, 3, 2, 2, 3, 3, 3, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 2, 3, 3, 2, 1, 2, 1, 1, 1, 2, 2, 3, 4, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 3, 2, 2, 3, 2, 2, 3, 4, 4, 4, 3, 2, 1, 0, 0, 0, 1, 2, 2, 1, 1, 2, 3, 3, 2, 1, 1, 1, 2, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3, 3, 3, 2, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 4, 4, 4, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3]
	alexa@ubuntu-xenial:0x02-hmm$
	
```






**Repo:**


* GitHub repository: `holbertonschool-machine_learning`
* Directory: `unsupervised_learning/hmm`
* File: `4-viterbi.py`










 Help
 




×
#### Students who are done with "4\. The Viretbi Algorithm"

















 Review your work
 




×
#### Correction of "4\. The Viretbi Algorithm"







Start a new test
Close


















 Requirement success
 


 Requirement fail
 




 Code success
 


 Code fail
 




 Efficiency success
 


 Efficiency fail
 




 Text answer success
 


 Text answer fail
 




 Skipped \- Previous check failed
 








 QA Review
 




×
#### 4\. The Viretbi Algorithm












##### Commit used:


* **User:**  \-\-\-
* **URL:** Click here
* **ID:** `---`
* **Author:** \-\-\-
* **Subject:** *\-\-\-*
* **Date:** \-\-\-














**1\.95/3** 
pts









### 5\. The Backward Algorithm




 mandatory
 











 Score: 65\.00% (Checks completed: 100\.00%)
 


Write the function `def backward(Observation, Emission, Transition, Initial):` that performs the backward algorithm for a hidden markov model:


* `Observation` is a `numpy.ndarray` of shape `(T,)` that contains the index of the observation
	+ `T` is the number of observations
* `Emission` is a `numpy.ndarray` of shape `(N, M)` containing the emission probability of a specific observation given a hidden state
	+ `Emission[i, j]` is the probability of observing `j` given the hidden state `i`
	+ `N` is the number of hidden states
	+ `M` is the number of all possible observations
* `Transition` is a 2D `numpy.ndarray` of shape `(N, N)` containing the transition probabilities
	+ `Transition[i, j]` is the probability of transitioning from the hidden state `i` to `j`
* `Initial` a `numpy.ndarray` of shape `(N, 1)` containing the probability of starting in a particular hidden state
* Returns: `P, B`, or `None, None` on failure
	+ `P`is the likelihood of the observations given the model
	+ `B` is a `numpy.ndarray` of shape `(N, T)` containing the backward path probabilities
		- `B[i, j]` is the probability of generating the future observations from hidden state `i` at time `j`



```
alexa@ubuntu-xenial:0x02-hmm$ cat 5-main.py
	#!/usr/bin/env python3
	
	import numpy as np
	backward = __import__('5-backward').backward
	
	if __name__ == '__main__':
		np.random.seed(1)
		Emission = np.array([[0.90, 0.10, 0.00, 0.00, 0.00, 0.00],
							 [0.40, 0.50, 0.10, 0.00, 0.00, 0.00],
							 [0.00, 0.25, 0.50, 0.25, 0.00, 0.00],
							 [0.00, 0.00, 0.05, 0.70, 0.15, 0.10],
							 [0.00, 0.00, 0.00, 0.20, 0.50, 0.30]])
		Transition = np.array([[0.60, 0.39, 0.01, 0.00, 0.00],
							   [0.20, 0.50, 0.30, 0.00, 0.00],
							   [0.01, 0.24, 0.50, 0.24, 0.01],
							   [0.00, 0.00, 0.15, 0.70, 0.15],
							   [0.00, 0.00, 0.01, 0.39, 0.60]])
		Initial = np.array([0.05, 0.20, 0.50, 0.20, 0.05])
		Hidden = [np.random.choice(5, p=Initial)]
		for _ in range(364):
			Hidden.append(np.random.choice(5, p=Transition[Hidden[-1]]))
		Hidden = np.array(Hidden)
		Observations = []
		for s in Hidden:
			Observations.append(np.random.choice(6, p=Emission[s]))
		Observations = np.array(Observations)
		P, B = backward(Observations, Emission, Transition, Initial.reshape((-1, 1)))
		print(P)
		print(B)
	alexa@ubuntu-xenial:0x02-hmm$ ./5-main.py
	1.7080966131859631e-214
	[[1.28912952e-215 6.12087935e-212 1.00555701e-211 ... 6.75000000e-005
	  0.00000000e+000 1.00000000e+000]
	 [3.86738856e-214 2.69573528e-212 4.42866330e-212 ... 2.02500000e-003
	  0.00000000e+000 1.00000000e+000]
	 [6.44564760e-214 5.15651808e-213 8.47145100e-213 ... 2.31330000e-002
	  2.70000000e-002 1.00000000e+000]
	 [1.93369428e-214 0.00000000e+000 0.00000000e+000 ... 6.39325000e-002
	  1.15000000e-001 1.00000000e+000]
	 [1.28912952e-215 0.00000000e+000 0.00000000e+000 ... 5.77425000e-002
	  2.19000000e-001 1.00000000e+000]]
	alexa@ubuntu-xenial:0x02-hmm$
	
```






**Repo:**


* GitHub repository: `holbertonschool-machine_learning`
* Directory: `unsupervised_learning/hmm`
* File: `5-backward.py`










 Help
 




×
#### Students who are done with "5\. The Backward Algorithm"

















 Review your work
 




×
#### Correction of "5\. The Backward Algorithm"







Start a new test
Close


















 Requirement success
 


 Requirement fail
 




 Code success
 


 Code fail
 




 Efficiency success
 


 Efficiency fail
 




 Text answer success
 


 Text answer fail
 




 Skipped \- Previous check failed
 








 QA Review
 




×
#### 5\. The Backward Algorithm












##### Commit used:


* **User:**  \-\-\-
* **URL:** Click here
* **ID:** `---`
* **Author:** \-\-\-
* **Subject:** *\-\-\-*
* **Date:** \-\-\-














**1\.95/3** 
pts









### 6\. The Baum\-Welch Algorithm




 mandatory
 











 Score: 43\.33% (Checks completed: 66\.67%)
 


Write the function `def baum_welch(Observations, Transition, Emission, Initial, iterations=1000):` that performs the Baum\-Welch algorithm for a hidden markov model:


* `Observations` is a `numpy.ndarray` of shape `(T,)` that contains the index of the observation
	+ `T` is the number of observations
* `Transition` is a `numpy.ndarray` of shape `(M, M)` that contains the initialized transition probabilities
	+ `M` is the number of hidden states
* `Emission` is a `numpy.ndarray` of shape `(M, N)` that contains the initialized emission probabilities
	+ `N` is the number of output states
* `Initial` is a `numpy.ndarray` of shape `(M, 1)` that contains the initialized starting probabilities
* `iterations` is the number of times expectation\-maximization should be performed
* Returns: the converged `Transition, Emission`, or `None, None` on failure



```
alexa@ubuntu-xenial:0x02-hmm$ cat 6-main.py
	#!/usr/bin/env python3
	
	import numpy as np
	baum_welch = __import__('6-baum_welch').baum_welch
	
	if __name__ == '__main__':
		np.random.seed(1)
		Emission = np.array([[0.90, 0.10, 0.00],
							 [0.40, 0.50, 0.10]])
		Transition = np.array([[0.60, 0.4],
							   [0.30, 0.70]])
		Initial = np.array([0.5, 0.5])
		Hidden = [np.random.choice(2, p=Initial)]
		for _ in range(364):
			Hidden.append(np.random.choice(2, p=Transition[Hidden[-1]]))
		Hidden = np.array(Hidden)
		Observations = []
		for s in Hidden:
			Observations.append(np.random.choice(3, p=Emission[s]))
		Observations = np.array(Observations)
		T_test = np.ones((2, 2)) / 2
		E_test = np.abs(np.random.randn(2, 3))
		E_test = E_test / np.sum(E_test, axis=1).reshape((-1, 1))
		T, E = baum_welch(Observations, T_test, E_test, Initial.reshape((-1, 1)))
		print(np.round(T, 2))
		print(np.round(E, 2))
	alexa@ubuntu-xenial:0x02-hmm$ ./6-main.py
	[[0.81 0.19]
	 [0.28 0.72]]
	[[0.82 0.18 0.  ]
	 [0.26 0.58 0.16]]
	alexa@ubuntu-xenial:0x02-hmm$
	
```

*With very little data (only 365 observations), we have been able to get a pretty good estimate of the transition and emission probabilities. We have not used a larger sample size in this example because our implementation does not utilize logarithms to handle values approaching 0 with the increased sequence length*







**Repo:**


* GitHub repository: `holbertonschool-machine_learning`
* Directory: `unsupervised_learning/hmm`
* File: `6-baum_welch.py`










 Help
 




×
#### Students who are done with "6\. The Baum\-Welch Algorithm"

















 Review your work
 




×
#### Correction of "6\. The Baum\-Welch Algorithm"







Start a new test
Close


















 Requirement success
 


 Requirement fail
 




 Code success
 


 Code fail
 




 Efficiency success
 


 Efficiency fail
 




 Text answer success
 


 Text answer fail
 




 Skipped \- Previous check failed
 








 QA Review
 




×
#### 6\. The Baum\-Welch Algorithm












##### Commit used:


* **User:**  \-\-\-
* **URL:** Click here
* **ID:** `---`
* **Author:** \-\-\-
* **Subject:** *\-\-\-*
* **Date:** \-\-\-














**1\.3/3** 
pts









[Previous project](/projects/2330)  



 Next project 







### Score

![Project badge](/assets/pathway/004_color-d2fbcfb42ba7481834896ecc89a6f0ae65762b3c1096691dd0f820f7f29e3389.png)83\.83%Good job! You can still grab some points!

Please review **all the tasks** before you start the peer review.

Review all the tasks


### Score

![Project badge](/assets/pathway/004_color-d2fbcfb42ba7481834896ecc89a6f0ae65762b3c1096691dd0f820f7f29e3389.png)83\.83%Good job! You can still grab some points!

Please review **all the tasks** before you start the peer review.

Review all the tasks



### Tasks list




* [Mandatory](#mandatory)
* [Advanced](#advanced)





0\. `Markov Chain`
**100\.00%**


1\. `Regular Chains`
**100\.00%**


2\. `Absorbing Chains`
**100\.00%**


3\. `The Forward Algorithm`
**65\.00%**


4\. `The Viretbi Algorithm`
**65\.00%**


5\. `The Backward Algorithm`
**65\.00%**


6\. `The Baum-Welch Algorithm`
**43\.33%**









×#### Recommended Sandboxes

New sandbox * 
* US East (N. Virginia)
* Ubuntu 18\.04
* Ubuntu 22\.04
* 
* South America (São Paulo)
* Ubuntu 18\.04
* Ubuntu 22\.04
* 
* Europe (Paris)
* Ubuntu 18\.04
* Ubuntu 22\.04
* 
* Asia Pacific (Sydney)
* Ubuntu 18\.04
* Ubuntu 22\.04
No sandboxes yet!





